# -*- coding: utf-8 -*-
"""MLCW.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sssjdak1HyEv4wSKIfuWEcr3VrOABSza
"""

import pandas as pd

# Load the dataset,
data = pd.read_csv("/content/bank-additional-full.csv", sep = ';')
display(data.head()) # Display the very first few rows of the dataset.

display(data.tail()) # Display the very last few rows of the dataset.

# Read the metadata file,
with open('/content/bank-additional-names.txt', 'r') as file:
    column_descriptions = file.readlines()

# Display the very first few lines of the metadata,
print("\nColumn Descriptions:")
with open('bank-additional-names.txt', 'r') as file:
    for line in file:
        print(line.strip())

# Explore the dataset,

# Check the first few rows of the dataset(.csv file)  (Already checked)

# Get a summary of the dataset,
print("\nDataset Summary:")
print(data.info())

# Check for missing values (If there any),
print("\nMissing Values:")
print(data.isnull().sum())

# Check for duplicate values (If there any),
print("\nDuplicate Values:")
print(data.duplicated().sum())

# Statistical summary,
print("\nStatistical Summary:")
print(data.describe())

# Visualize the dataset,

import matplotlib.pyplot as plt
import seaborn as sns

# Example: Visualize a categorical column (Job),
sns.countplot(data['job'])
plt.xticks(rotation=45)
plt.show()

# Example: Visualize a numerical column (Age),

sns.histplot(data['age'], bins=20, kde=True)
plt.xticks(rotation=45)
plt.show()

# Identify the features of the dataset,

# Identify the target variables, (Check for unique values)
print(data['y'].value_counts()) # As for the output, It indicates a Binary Classification problem.

# List categorical columns,
categorical_features = data.select_dtypes(include=['object']).columns
print("\nCategorical Features:", categorical_features)

# Check unique values in a sample column
print(data['job'].unique())

# List numerical columns,
numerical_features = data.select_dtypes(include=['int64', 'float64']).columns
print("\nNumerical Features:", numerical_features)

# Check statistics of a sample numerical column,
print(data['age'].describe())

# Data Pre-Processing,

# Handle missing values, (In txt it says there are some missing values in categorical columns and not in numerical ones.)

# Check for missing values,
print("\nMissing Values:")
print(data.isnull().sum())

# Look for placeholder values in categorical attributes in the dataset, (Should be displayed as "Unknown" as it given under the txt file)
for column in data.select_dtypes(include='object').columns:
    print(f"\nUnique values in {column}:", data[column].unique()) # In output, those are the null values which given by placeholder "Unknown".


# Deletion of rows which include null values,
for column in data.select_dtypes(include='object').columns:
    data = data[~(data[column] == 'unknown')]

print("\n\n\nNew dataset after the deletion of Null-values")
# Verify changes,
for column in data.select_dtypes(include='object').columns:
    print(f"\nUnique values in {column}:", data[column].unique()) # All the null values are erased.


# Encode Categorical variables, (ML algorithms deals with only Numerical features)
from sklearn.preprocessing import LabelEncoder

# Label Encoding for Random Forest,
data_rf = data.copy() # Wanna create a copy of the dataset specifically for Random Forest.

# Apply Label Encoding for all categorical features,
label_encoder = LabelEncoder()
for column in data_rf.select_dtypes(include=['object']).columns:
    data_rf[column] = label_encoder.fit_transform(data_rf[column])
# Transformed dataset,
print("\n\n\nTransformed dataset for RF:")
print(data_rf.head)

# One-Hot Encoding for Neural Network,
data_nn = data.copy() # Wanna create another copy of the dataset for Neural Network.

# Apply One-Hot Enocoding for all categorical features,
data_nn = pd.get_dummies(data_nn, columns=data_nn.select_dtypes(include=['object']).columns, drop_first=True)
# Transformed dataset,
print("\n\nTransformed dataset for NN:")
print(data_nn.head)

# Save both versions of those datasets,
data_rf.to_csv('data_rf_preprocessed.csv', index=False) # Transformed RF dataset
data_nn.to_csv('data_nn_preprocessed.csv', index=False) # Transformed NN Dataset


# Feature Scaling,
# Random Forest, (No need of Scaling)

# The original numeric features of the dataset,
numerical_columns = ['age', 'duration', 'campaign', 'pdays', 'previous',
                     'emp.var.rate', 'cons.price.idx', 'cons.conf.idx',
                     'euribor3m', 'nr.employed']

print("\nPresence of Numerical columns:")
# Verify their presense (To double-check),
print(data_nn[numerical_columns].head())

# Apply Standard Scaling,
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

# Scale the Numerical columns,
data_nn[numerical_columns] = scaler.fit_transform(data_nn[numerical_columns])

# Normalize the Numerical columns,
from sklearn.preprocessing import MinMaxScaler # Not useful.
scaler = MinMaxScaler()
data_nn[numerical_columns] = scaler.fit_transform(data_nn[numerical_columns])

# Check scaled values,
print("\n\nScaled Numerical columns:")
print(data_nn[numerical_columns].head())
# Check the normalized values,
print("\n\nNormalized Numerical columns:") # Not useful.
print(data_nn[numerical_columns].head())
# No need to scale One-Hot Encoded features as because they have already been in Binary (0,1)....

# Save the Scaled dataset,
data_nn.to_csv('data_nn_scaled.csv', index=False) # Scaled and Encoded dataset for NN.

# Recheck the dataset created for RF to ensure the dataset is fully pre-processed,
print(data_rf.info())
# From now onwards use the following datasets ;
      # For Random Forest Model - Use data_rf_preprocessed.csv
      # For Neural Network Model - Use data_nn_scaled.csv

# Split the Datasets into Training and Testing Sets,

from sklearn.model_selection import train_test_split

# Reload datasets which saved earlier,
data_rf = pd.read_csv('data_rf_preprocessed.csv')
data_nn = pd.read_csv('data_nn_scaled.csv')

print(data_nn.columns)  # To correctly identify whats the new name of the predicted variable after pre-processing.


# Define Features and Target variables,

# For RF,
X_rf = data_rf.drop('y', axis=1) # Features
y_rf = data_rf['y'] # Target "y"

# For NN,
X_nn = data_nn.drop('y_yes', axis=1) # Features
y_nn = data_nn['y_yes'] # Target "y"                           # Check whether the datasets are correctly named..

# RF dataset split,
X_train_rf, X_test_rf, Y_train_rf, Y_test_rf = train_test_split(X_rf, y_rf, test_size=0.2, random_state=42)
# NN dataset split,
X_train_nn, X_test_nn, Y_train_nn, Y_test_nn = train_test_split(X_nn, y_nn, test_size=0.2, random_state=42)

# Check the sizes of training and testing to ensure the splitting is done correctly,

# For RF,
print("\nRandom-Forest Dataset Split for Training set: ", X_train_rf.shape, Y_train_rf.shape)
print("Random-Forest Dataset Split for Testing set: ", X_test_rf.shape, Y_test_rf.shape)    # 20; No. of features after Encoding.... 24390; No. of samples....
# For NN,
print("Neural-Network Dataset Split for Training set: ", X_train_nn.shape, Y_train_nn.shape)
print("Neural-Network Dataset Split for Testing set: ", X_test_nn.shape, Y_test_nn.shape)   # Same as above...


# To check whether the Target variable balanced across the Testing and Training, (RF)
print("\nTraining set target distribution:")
print(Y_train_rf.value_counts())
print("\nTesting set target distribution:")
print(Y_test_rf.value_counts())

# Resampling because the Target variable is inbalanced across the Training split,
from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_rf_train_resampled, Y_rf_train_resampled = smote.fit_resample(X_train_rf, Y_train_rf)

print("\nResampled Training set Target distribution (RF):")
print(Y_rf_train_resampled.value_counts()) # Now its balanced...
# No need to resample the Testing split because its the original set.

# To check whether the Target variable balanced across the Testing and Training, (NN)
print("\nTraining set target distribution:")
print(Y_train_nn.value_counts())
print("\nTesting set target distribution:")
print(Y_test_nn.value_counts())

# Resampling,
from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_nn_train_resampled, y_nn_train_resampled = smote.fit_resample(X_train_nn, Y_train_nn)

# Check the new class distribution,
print("\nResampled Training Target Distribution (NN):")
print(y_nn_train_resampled.value_counts()) # Now its balanced...
# As same as above, no need to resample the Testing split as because its the original set.

# Model Building Part,



# Random-Forest Model,
        # Here, use the RF datasets; X_rf_train_resampled, Y_rf_train_resampled, X_test_rf, Y_test_rf

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score
from imblearn.over_sampling import SMOTE

# Resampling again, (Due to struggling predicting the minority class)
smote = SMOTE(random_state=42)
X_rf_train_resampled, Y_rf_train_resampled = smote.fit_resample(X_rf_train_resampled, Y_rf_train_resampled)

# Random Forest Classifier,
from sklearn.utils.class_weight import compute_class_weight
import numpy as np

class_weights = compute_class_weight('balanced', classes=np.unique(Y_rf_train_resampled), y=Y_rf_train_resampled)
weights_dict = dict(zip(np.unique(Y_rf_train_resampled), class_weights))  # Assign higher weights for minority classes...
rf_classifier = RandomForestClassifier(class_weight=weights_dict, random_state=42)


# Train the model,  (Adjusting the hyperparameters of RF to improve minority class performance...)
rf_classifier = RandomForestClassifier(n_estimators=200,max_depth=10,min_samples_split=10,min_samples_leaf=5,class_weight='balanced',random_state=42
)
rf_classifier.fit(X_rf_train_resampled, Y_rf_train_resampled)

from sklearn.metrics import precision_recall_curve

y_rf_proba = rf_classifier.predict_proba(X_test_rf)[:, 1]
precision, recall, thresholds = precision_recall_curve(Y_test_rf, y_rf_proba)

# Choose a threshold that balances precision and recall
optimal_threshold = thresholds[np.argmax(2 * (precision * recall) / (precision + recall))]
y_rf_pred_adjusted = (y_rf_proba >= optimal_threshold).astype(int)


# Predict on the Test set,
y_rf_pred = rf_classifier.predict(X_test_rf)

# Evaluate the model,
from sklearn.metrics import roc_auc_score
y_rf_proba = rf_classifier.predict_proba(X_test_rf)[:, 1]
roc_auc = roc_auc_score(Y_test_rf, y_rf_proba)
print("\nROC-AUC Score:", roc_auc)

print("\nRandom Forest Accuracy:", accuracy_score(Y_test_rf, y_rf_pred))
print("\nClassification Report:\n", classification_report(Y_test_rf, y_rf_pred))
print("\nConfusion Matrix:\n", confusion_matrix(Y_test_rf, y_rf_pred))

# Neural-Network Model,
      # Here, use the NN datasets; X_nn_train_resampled, y_nn_train_resampled, X_test_nn, Y_test_nn

import os
import random
import numpy as np
import tensorflow as tf

# Set seeds for reproducibility
random.seed(42)
np.random.seed(42)
tf.random.set_seed(42)

# Ensure TensorFlow uses deterministic operations (if applicable)
os.environ['TF_DETERMINISTIC_OPS'] = '1'


import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model, save_model
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import to_categorical

# Convert Target to categorical (one-hot encoding),
y_nn_train_resampled_cat = to_categorical(y_nn_train_resampled)
y_nn_test_cat = to_categorical(Y_test_nn)

#  Neural Network,
nn_model = Sequential([
    Dense(64, activation='relu', input_shape=(X_nn_train_resampled.shape[1],)),
    Dense(32, activation='relu'),
    Dense(2, activation='softmax')  # Adjust for binary classification
])
# Compile the model,
nn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

from tensorflow.keras.callbacks import EarlyStopping

# Early stopping to control overfitting,
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Train the model using above mentioned sets,
nn_model.fit(X_nn_train_resampled,
    y_nn_train_resampled_cat,
    epochs=10,
    batch_size=32,
    validation_split=0.2,
    callbacks=[early_stopping],
    verbose=2)

# Save model,     # Because accuracies is getting changed in each run thats why doing these things...
tf.keras.models.save_model(nn_model, 'neural_network_model.keras')
# Reload model,
model = tf.keras.models.load_model('neural_network_model.keras')

# Evaluate the model,
nn_loss, nn_accuracy = nn_model.evaluate(X_test_nn, y_nn_test_cat)
print("\nNeural Network Accuracy:", nn_accuracy)

# Evaluate multiple times to verify consistency,
for i in range(3):
    loss, accuracy = model.evaluate(X_test_nn, y_nn_test_cat, verbose=0)
    print(f"Run {i+1} - Test Accuracy: {accuracy:.4f}")

# Well after all these things, it should get an accuracy of 88.5%....
# Hence, Neural Network is working well...


from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Get predicted probabilities and binary predictions from the model,
y_nn_pred_proba = model.predict(X_test_nn)  # Predicted probabilities
y_nn_pred = np.argmax(y_nn_pred_proba, axis=1)  # Predicted class labels

# Convert one-hot encoded true labels to single column format,
y_nn_test_actual = np.argmax(y_nn_test_cat, axis=1)

# Calculate Metrics,
accuracy_nn = accuracy_score(y_nn_test_actual, y_nn_pred)
precision_nn = precision_score(y_nn_test_actual, y_nn_pred, average=None)
recall_nn = recall_score(y_nn_test_actual, y_nn_pred, average=None)
f1_nn = f1_score(y_nn_test_actual, y_nn_pred, average=None)
roc_auc_nn = roc_auc_score(y_nn_test_cat, y_nn_pred_proba, multi_class='ovr')  # One-vs-Rest for binary


# Confusion Matrix,
conf_matrix_nn = confusion_matrix(y_nn_test_actual, y_nn_pred)
print("\nConfusion Matrix:\n", conf_matrix_nn)

# Classification Report,
report_nn = classification_report(y_nn_test_actual, y_nn_pred, target_names=['Class 0', 'Class 1'])
print("\nClassification Report:\n", report_nn)

# Comparation of two models (RF and NN) using Optimal Evaluation Metrics,

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Random Forest Evaluation,
# Assuming 'y_rf_pred' and 'Y_test_rf' are defined from the RF model section
rf_accuracy = accuracy_score(Y_test_rf, y_rf_pred)
rf_precision = precision_score(Y_test_rf, y_rf_pred)
rf_recall = recall_score(Y_test_rf, y_rf_pred)
rf_f1 = f1_score(Y_test_rf, y_rf_pred)
rf_roc_auc = roc_auc_score(Y_test_rf, y_rf_proba)  # Assuming 'y_rf_proba' is available

# Neural Network Evaluation,
# Assuming 'nn_model', 'X_test_nn', and 'y_nn_test_cat' are defined from the NN model section
y_nn_pred_proba = nn_model.predict(X_test_nn)
y_nn_pred = np.argmax(y_nn_pred_proba, axis=1)  # Convert probabilities to class labels.
nn_accuracy = accuracy_score(Y_test_nn, y_nn_pred)
nn_precision = precision_score(Y_test_nn, y_nn_pred)
nn_recall = recall_score(Y_test_nn, y_nn_pred)
nn_f1 = f1_score(Y_test_nn, y_nn_pred)
nn_roc_auc = roc_auc_score(Y_test_nn, y_nn_pred_proba[:, 1]) # ROC-AUC for class 1


# Print Results,
print("\n--- Model Comparison ---")
print("Metric\t\tRandom Forest\tNeural Network")
print(f"Accuracy\t\t{rf_accuracy:.4f}\t\t{nn_accuracy:.4f}")
print(f"Precision\t\t{rf_precision:.4f}\t\t{nn_precision:.4f}")
print(f"Recall\t\t\t{rf_recall:.4f}\t\t{nn_recall:.4f}")
print(f"F1-score\t\t{rf_f1:.4f}\t\t{nn_f1:.4f}")
print(f"ROC-AUC\t\t\t{rf_roc_auc:.4f}\t\t{nn_roc_auc:.4f}")


import matplotlib.pyplot as plt
import numpy as np


# Group metrics for each model
rf_metrics = [rf_accuracy, rf_precision, rf_recall, rf_f1, rf_roc_auc]
nn_metrics = [nn_accuracy, nn_precision, nn_recall, nn_f1, nn_roc_auc]

# Metric labels,
metric_labels = ['Accuracy', 'Precision', 'Recall', 'F1-score', 'ROC-AUC']

# Set up the plot,
x = np.arange(len(metric_labels))  # the label locations..
width = 0.35

fig, ax = plt.subplots()
rects1 = ax.bar(x - width/2, rf_metrics, width, label='Random Forest')
rects2 = ax.bar(x + width/2, nn_metrics, width, label='Neural Network')

ax.set_ylabel('Score')
ax.set_title('Model Comparison')
ax.set_xticks(x)
ax.set_xticklabels(metric_labels)
ax.legend()

# Add value labels on top of the bars,
def autolabel(rects):
    """Attach a text label above each bar in *rects*, displaying its height."""
    for rect in rects:
        height = rect.get_height()
        ax.annotate('{:.4f}'.format(height),
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')

print("\n\n\nVisualization of accuracies on each model:")


autolabel(rects1)
autolabel(rects2)

fig.tight_layout()
plt.show()